{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCTU_DL_HW2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd406dQmkPWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdmgomkQ4Ud8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph\n",
        "num_class = 10\n",
        "unit = 512\n",
        "lr = 0.0001\n",
        "x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
        "y = tf.placeholder(tf.float32, [None, num_class])\n",
        "\n",
        "weight1 = tf.Variable(tf.random_normal([28*28,unit] , stddev=0.3))\n",
        "bias1    =tf.Variable(tf.random_normal([unit], stddev=0.3))\n",
        "hidden1  =tf.add(tf.matmul(x, weight1), bias1)\n",
        "act1   =tf.nn.relu(hidden1)\n",
        "\n",
        "\n",
        "weight2 = tf.Variable(tf.random_normal([unit,unit], stddev=0.3))\n",
        "bias2    =tf.Variable(tf.random_normal([unit], stddev=0.3))\n",
        "hidden2  =tf.add(tf.matmul(act1, weight2), bias2)\n",
        "act2   =tf.nn.relu(hidden2)\n",
        "\n",
        "weight3 = tf.Variable(tf.random_normal([unit,num_class], stddev=0.3))\n",
        "bias3    =tf.Variable(tf.random_normal([num_class], stddev=0.3))\n",
        "hidden3  =tf.add(tf.matmul(act2, weight3), bias3)\n",
        "y_   =tf.nn.softmax(hidden3)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y,\n",
        "                    logits = y_))\n",
        "\n",
        "'''\n",
        "l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.5, scope=None)\n",
        "\n",
        "regularizer_l1= tf.contrib.layers.apply_regularization(l1_regularizer, [weight1, weight2, weight3, weight4])\n",
        "\n",
        "cost = cost +regularizer_l1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "l2_regularizer = tf.contrib.layers.l2_regularizer(scale=0.3, scope=None)\n",
        "\n",
        "regularizer_l2= tf.contrib.layers.apply_regularization(l2_regularizer, [weight1])\n",
        "\n",
        "cost = cost + regularizer_l2 \n",
        "\n",
        "'''\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate= lr ).minimize(cost)\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)\n",
        "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))  \n",
        "\n",
        "\n",
        "init=tf.global_variables_initializer()\n",
        "#session_conf=tf.ConfigProto(intra_op_parallelism_threads=4)\n",
        "#sess=tf.Session(config=session_conf)\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "epoach= 100\n",
        "batch = 150\n",
        "val_acc_list = []\n",
        "val_loss_list = []\n",
        "test_acc_list = []\n",
        "test_loss_list = []\n",
        "for i in range(epoach):\n",
        "    train_acc=0\n",
        "    for _ in range(int(mnist.train.num_examples/batch)): \n",
        "        batch_x , batch_y =mnist.train.next_batch(batch)\n",
        "        [c,loss,acc]=sess.run([optimizer,cost,accuracy],feed_dict={x: batch_x , y: batch_y})\n",
        "    loss,acc=sess.run([cost,accuracy ] ,feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
        "    print(i,'validation accuracy :',acc)\n",
        "    val_acc_list.append(acc)\n",
        "    val_loss_list.append(loss)\n",
        "    loss, acc=sess.run([cost,accuracy ], feed_dict = {x: mnist.test.images, y: mnist.test.labels} )    \n",
        "    print(i,'test accuracy :',acc)\n",
        "    test_acc_list.append(acc)\n",
        "    test_loss_list.append(loss)\n",
        "\n",
        "w1 = tf.Print(weight1,[weight1])\n",
        "w1 = np.array(w1.eval(session=sess))\n",
        "w2 = tf.Print(weight2,[weight2])\n",
        "w2 = np.array(w2.eval(session=sess))\n",
        "\n",
        "w3 = tf.Print(weight3,[weight3])\n",
        "w3 = np.array(w3.eval(session=sess))\n",
        "sess.close() \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeySu4tqpGi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "      \n",
        "\n",
        "plt.figure(2)\n",
        "w1 = np.ravel(w1)\n",
        "plt.title(\"histogram of h1\")\n",
        "plt.xlabel(\"value\")\n",
        "plt.ylabel(\"number\")\n",
        "plt.hist(w1, bins=100, range=(-3, 3))\n",
        "\n",
        "plt.figure(3)\n",
        "w2 = np.ravel(w2)\n",
        "plt.title(\"histogram of h2\")\n",
        "plt.xlabel(\"value\")\n",
        "plt.ylabel(\"number\")\n",
        "plt.hist(w2, bins=100, range=(-3, 3))\n",
        "\n",
        "plt.figure(4)\n",
        "w3 = np.ravel(w3)\n",
        "plt.title(\"histogram of h3\")\n",
        "plt.xlabel(\"value\")\n",
        "plt.ylabel(\"number\")\n",
        "plt.hist(w3, bins=100, range=(-3, 3))\n",
        "\n",
        "plt.show()  \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjreeYZ0pGla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.plot(range(epoach) , val_acc_list , label ='val')\n",
        "plt.plot(range(epoach) , test_acc_list , label='test')\n",
        "plt.show()\n",
        "plt.plot(range(epoach) ,val_loss_list  ,label='val')\n",
        "plt.plot(range(epoach) , test_loss_list , label='test')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulFRzzsYpGoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R870jecMDnmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxvf cifar-10-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pzbJc0FDWA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "import os  \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow.contrib.slim as slim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQg9B3vlDV-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def one_hot(x,n):\n",
        "    x=np.array(x)\n",
        "    assert x.ndim ==1\n",
        "    return np.eye(n)[x]\n",
        "\n",
        "def load_batch_cifar10(filename):\n",
        "    path =os.path.join('./cifar-10-batches-py/' , filename)\n",
        "    batch =np.load(path)\n",
        "    data=batch['data']/255.0\n",
        "    labels=one_hot(batch['labels'],n=10)\n",
        "    return data.astype('float32') , labels.astype('float32')\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def cifar10():\n",
        "    train_x=[]\n",
        "    train_y=[]\n",
        "    print(\"loading train...\")\n",
        "    for k in range(1,6):\n",
        "        dict_ =unpickle('./cifar-10-batches-py/data_batch_%d' % k )\n",
        "        train_x.append(dict_[b'data'])\n",
        "        train_y.append(one_hot (dict_[b'labels'] ,10))\n",
        "    train_x = np.concatenate(train_x, axis=0)  \n",
        "    train_y = np.concatenate(train_y, axis=0) \n",
        "    print(\"loading test...\")\n",
        "    dict_ = unpickle('./cifar-10-batches-py/test_batch')\n",
        "    print('load finished')\n",
        "    return train_x, train_y, np.array(dict_[b'data']), np.array(one_hot(dict_[b'labels'],10))\n",
        "Xtrain, Ytrain, Xtest, Ytest = cifar10()  \n",
        "print(Xtrain.shape , Ytrain.shape, Xtest.shape , Ytest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH9X24XRDV6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate=0.001\n",
        "batch=500\n",
        "entity=3072   #32*32*3\n",
        "classes=10\n",
        "num_epoch=100\n",
        "data_length = Xtrain.shape[0]\n",
        "\n",
        "x=tf.placeholder(tf.float32,[None,entity ])\n",
        "y=tf.placeholder(tf.float32,[None,classes])\n",
        "\n",
        "weights={\n",
        "     'w1' :tf.Variable(tf.random_normal([5,5,3,32])),  #feature map 1\n",
        "     'w2' :tf.Variable(tf.random_normal([5,5,32,64])) , #feature map 2\n",
        "     'w3' :tf.Variable(tf.random_normal([8*8*64,1024])), # full concent layer\n",
        "     'w4' :tf.Variable(tf.random_normal([1024 , classes]))  # output layer  \n",
        "       }\n",
        "biases={\n",
        "      'b1' :tf.Variable(tf.random_normal([32])) , #feature map 1\n",
        "      'b2'  :tf.Variable(tf.random_normal([64])), #feature map 2\n",
        "      'b3' :tf.Variable(tf.random_normal([1024])), # full concent layer\n",
        "      'b4' :tf.Variable(tf.random_normal([classes])) # output layer  \n",
        "        }\n",
        "def conv2d_step(x,w,b,stride=1):\n",
        "    x=tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding='SAME')\n",
        "    return tf.add(x,b)\n",
        "def detector_step(x):\n",
        "    return tf.nn.relu(x)\n",
        "def pool_step(x,stride=2,dtype='SAME'):\n",
        "    return tf.nn.max_pool(x, ksize=[1,stride,stride,1],strides=[1,stride,stride,1],padding='SAME')\n",
        "def convolution_net(x,w,b):\n",
        "    x=tf.reshape(x,shape=[-1, 32, 32, 3])\n",
        "    fearture_map1 = conv2d_step(x, weights['w1'], biases['b1'])\n",
        "    feature1=detector_step(fearture_map1)\n",
        "    hidden1=pool_step(feature1,stride=2,dtype='SAME')\n",
        "    fearture_map2 = conv2d_step(hidden1,weights['w2'],biases['b2'])\n",
        "    feature2=detector_step(fearture_map2)\n",
        "    hidden2=pool_step(feature2,stride=2,dtype='SAME')\n",
        "    \n",
        "    full_connect_layer1=tf.reshape(hidden2, [-1, weights['w3'].get_shape().as_list()[0]])\n",
        "    full_connect_layer1=tf.nn.relu(tf.add(tf.matmul(full_connect_layer1, weights['w3']),biases['b3']))\n",
        "    out = tf.add(tf.matmul(full_connect_layer1,weights['w4']),biases['b4'])\n",
        "    return out ,feature1 ,feature2 \n",
        "\n",
        "predict, feature1, feature2=convolution_net(x,weights,biases)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predict, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "prediction = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
        "print('construct model')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHMs_-qSnMtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv1,pool1,conv2, pool2=sess.run([feature1,hidden1,feature2,hidden1 ] ,\\\n",
        "                                  feed_dict={x:Xtrain[random_] ,y:Ytrain[random_]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jowmK9QbELln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "init=tf.global_variables_initializer()\n",
        "session_conf=tf.ConfigProto(intra_op_parallelism_threads=4)\n",
        "sess=tf.Session(config=session_conf)\n",
        "sess.run(init)\n",
        "print('start training')\n",
        "for i in range(num_epoch) :\n",
        "    for  j in range(int (data_length/batch)):\n",
        "        batch_x, batch_y = Xtrain[batch*j:batch*(j+1)],Ytrain[batch*j:batch*(j+1)]\n",
        "        sess.run(optimizer,feed_dict={x:batch_x,  y:batch_y})\n",
        "    print (i,'train loss and accuracy : ',sess.run([cost,accuracy],feed_dict={x:Xtrain,y:Ytrain}))\n",
        "    print(i,'test loss and accuracy : ',sess.run([cost,accuracy],feed_dict={x: Xtest ,y: Ytest}))\n",
        "\n",
        "random_ = [5,6,40,15,32]\n",
        "conv1,conv2=sess.run([feature1,feature2 ] ,\\\n",
        "                                  feed_dict={x:Xtrain[random_] ,y:Ytrain[random_]})\n",
        "#sess.close()\n",
        "image1 = conv1[1][0]\n",
        "image2 = conv1[2][0]\n",
        "image3 = conv2[2][0]\n",
        "image4 = conv2[0][0]\n",
        "#  \n",
        "plt.figure(1)  \n",
        "plt.imshow(image1)  \n",
        "plt.figure(2)  \n",
        "plt.imshow(image2)  \n",
        "\n",
        "plt.figure(3)  \n",
        "plt.imshow(image3)  \n",
        "plt.figure(4)  \n",
        "plt.imshow(image4)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-syOi_fVxCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv1,conv2=sess.run([feature1,feature2 ] ,\\\n",
        "                                  feed_dict={x:Xtrain[random_] ,y:Ytrain[random_]})\n",
        "#sess.close()\n",
        "image1 = conv1[0][0]\n",
        "image2 = conv1[0][0]\n",
        "image3 = conv2[0][0]\n",
        "image4 = conv2[0][0]\n",
        "#  \n",
        "plt.figure(1)  \n",
        "plt.imshow(image1)  \n",
        "plt.figure(2)  \n",
        "plt.imshow(image2)  \n",
        "\n",
        "plt.figure(3)  \n",
        "plt.imshow(image3)  \n",
        "plt.figure(4)  \n",
        "plt.imshow(image4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w78KD40hoBaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}